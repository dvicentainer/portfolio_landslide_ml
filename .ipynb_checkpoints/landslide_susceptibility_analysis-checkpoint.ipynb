{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# üèîÔ∏è An√°lise de Susceptibilidade a Deslizamentos de Terra\n",
    "\n",
    "## üìã Vis√£o Geral\n",
    "Este notebook implementa um modelo de machine learning para an√°lise de susceptibilidade a deslizamentos de terra utilizando dados geoespaciais. O estudo segue a metodologia **SEMMA** (Sample, Explore, Modify, Model, Assess).\n",
    "\n",
    "**Autor:** Denis Vicentainer  \n",
    "**Data:** 2024  \n",
    "**Objetivo:** Desenvolver modelos preditivos para identifica√ß√£o de √°reas suscept√≠veis a deslizamentos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## üì¶ Importa√ß√£o de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import_libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipula√ß√£o de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Dados geoespaciais\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import geopandas as gpd\n",
    "\n",
    "# Visualiza√ß√£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_curve, auc,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Estat√≠stica\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy import stats\n",
    "\n",
    "# Configura√ß√µes\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "semma_sample",
   "metadata": {},
   "source": [
    "---\n",
    "# üî¨ METODOLOGIA SEMMA\n",
    "\n",
    "## 1Ô∏è‚É£ SAMPLE - Amostragem dos Dados\n",
    "\n",
    "Nesta etapa, carregamos e preparamos os dados raster geoespaciais para an√°lise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_loading",
   "metadata": {},
   "source": [
    "### üìÇ Carregamento dos Dados Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir caminhos dos dados\n",
    "data_folder = 'data'\n",
    "file_path = os.path.join(data_folder, 'composite_bands4.tif')\n",
    "\n",
    "# Verificar se o arquivo existe\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"‚ùå Arquivo n√£o encontrado: {file_path}\")\n",
    "    print(\"üìÅ Arquivos dispon√≠veis na pasta data:\")\n",
    "    if os.path.exists(data_folder):\n",
    "        for file in os.listdir(data_folder):\n",
    "            print(f\"   - {file}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Arquivo encontrado: {file_path}\")\n",
    "\n",
    "# Nomes das bandas/vari√°veis\n",
    "band_names = [\n",
    "    'aspect', 'elevation', 'geology', 'landslide_scars', 'ndvi',\n",
    "    'plan_curv', 'profile_curv', 'slope', 'spi', 'twi'\n",
    "]\n",
    "\n",
    "print(f\"\\nüìä Vari√°veis a serem analisadas: {len(band_names)}\")\n",
    "for i, name in enumerate(band_names, 1):\n",
    "    print(f\"   {i:2d}. {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raster_to_dataframe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raster_to_dataframe(file_path, band_names):\n",
    "    \"\"\"\n",
    "    Converte dados raster para DataFrame pandas.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Caminho para o arquivo raster\n",
    "    band_names : list\n",
    "        Lista com nomes das bandas\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame com dados do raster\n",
    "    \"\"\"\n",
    "    with rasterio.open(file_path) as src:\n",
    "        # Ler todos os dados do raster\n",
    "        data = src.read()\n",
    "        \n",
    "        # Obter informa√ß√µes do raster\n",
    "        num_layers, height, width = data.shape\n",
    "        \n",
    "        print(f\"üìê Dimens√µes do raster:\")\n",
    "        print(f\"   - Bandas: {num_layers}\")\n",
    "        print(f\"   - Altura: {height} pixels\")\n",
    "        print(f\"   - Largura: {width} pixels\")\n",
    "        print(f\"   - Total de pixels: {height * width:,}\")\n",
    "        \n",
    "        # Reshape dos dados para formato tabular\n",
    "        reshaped_data = data.reshape(num_layers, -1).T\n",
    "        \n",
    "        # Criar DataFrame\n",
    "        df = pd.DataFrame(reshaped_data, columns=band_names)\n",
    "        \n",
    "        print(f\"\\n‚úÖ DataFrame criado com sucesso!\")\n",
    "        print(f\"   - Shape: {df.shape}\")\n",
    "        print(f\"   - Mem√≥ria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Carregar dados\n",
    "df_raw = raster_to_dataframe(file_path, band_names)\n",
    "\n",
    "# Visualizar primeiras linhas\n",
    "print(\"\\nüìã Primeiras 5 linhas dos dados:\")\n",
    "display(df_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_info",
   "metadata": {},
   "source": [
    "### üìä Informa√ß√µes B√°sicas dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_info",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informa√ß√µes b√°sicas do dataset\n",
    "print(\"üìä INFORMA√á√ïES B√ÅSICAS DO DATASET\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {df_raw.shape}\")\n",
    "print(f\"Colunas: {list(df_raw.columns)}\")\n",
    "print(f\"\\nTipos de dados:\")\n",
    "print(df_raw.dtypes)\n",
    "\n",
    "print(f\"\\nüìà ESTAT√çSTICAS DESCRITIVAS\")\n",
    "print(\"=\" * 50)\n",
    "display(df_raw.describe())\n",
    "\n",
    "print(f\"\\nüîç VALORES AUSENTES\")\n",
    "print(\"=\" * 50)\n",
    "missing_info = df_raw.isnull().sum()\n",
    "missing_percent = (missing_info / len(df_raw)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Valores Ausentes': missing_info,\n",
    "    'Percentual (%)': missing_percent\n",
    "})\n",
    "display(missing_df[missing_df['Valores Ausentes'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "semma_explore",
   "metadata": {},
   "source": [
    "---\n",
    "## 2Ô∏è‚É£ EXPLORE - An√°lise Explorat√≥ria dos Dados\n",
    "\n",
    "Nesta etapa, exploramos as caracter√≠sticas dos dados, identificamos padr√µes e relacionamentos entre as vari√°veis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_cleaning_prep",
   "metadata": {},
   "source": [
    "### üßπ Prepara√ß√£o Inicial dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_types",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar c√≥pia dos dados para manipula√ß√£o\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Converter tipos de dados apropriados\n",
    "print(\"üîÑ Convertendo tipos de dados...\")\n",
    "\n",
    "# Vari√°veis categ√≥ricas\n",
    "df['geology'] = df['geology'].astype('float64')\n",
    "df['landslide_scars'] = df['landslide_scars'].astype('float64')\n",
    "df['elevation'] = df['elevation'].astype('float64')\n",
    "\n",
    "print(\"‚úÖ Tipos de dados convertidos!\")\n",
    "print(\"\\nTipos atuais:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outlier_treatment",
   "metadata": {},
   "source": [
    "### üéØ Tratamento de Outliers e Valores Inv√°lidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean_outliers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_outliers(df):\n",
    "    \"\"\"\n",
    "    Remove outliers e valores inv√°lidos baseado no conhecimento do dom√≠nio.\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    print(\"üßπ Limpando outliers e valores inv√°lidos...\")\n",
    "    print(f\"Shape inicial: {df_clean.shape}\")\n",
    "    \n",
    "    # Definir ranges v√°lidos para cada vari√°vel\n",
    "    valid_ranges = {\n",
    "        'elevation': (-30, 942),\n",
    "        'aspect': (-1, 360),\n",
    "        'geology': (0, float('inf')),  # Valores >= 0\n",
    "        'landslide_scars': (-1, float('inf')),  # Valores >= -1\n",
    "        'ndvi': (-1, 1),\n",
    "        'plan_curv': (-8.04696, 6.0631),\n",
    "        'profile_curv': (-8.8354, 10.5086),\n",
    "        'slope': (0, 64.991),\n",
    "        'spi': (-14.5964, 7.18534),\n",
    "        'twi': (-6907.75, 12986.3)\n",
    "    }\n",
    "    \n",
    "    # Aplicar limpeza\n",
    "    for col, (min_val, max_val) in valid_ranges.items():\n",
    "        if col in df_clean.columns:\n",
    "            before_count = df_clean[col].notna().sum()\n",
    "            \n",
    "            if col == 'geology':\n",
    "                df_clean.loc[df_clean[col] < 0, col] = np.nan\n",
    "            elif col == 'landslide_scars':\n",
    "                df_clean.loc[df_clean[col] < -1, col] = np.nan\n",
    "            else:\n",
    "                df_clean.loc[~df_clean[col].between(min_val, max_val), col] = np.nan\n",
    "            \n",
    "            after_count = df_clean[col].notna().sum()\n",
    "            removed = before_count - after_count\n",
    "            \n",
    "            if removed > 0:\n",
    "                print(f\"   {col}: {removed:,} valores inv√°lidos removidos\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Limpeza conclu√≠da!\")\n",
    "    return df_clean\n",
    "\n",
    "# Aplicar limpeza\n",
    "df_clean = clean_outliers(df)\n",
    "\n",
    "# Mostrar estat√≠sticas ap√≥s limpeza\n",
    "print(\"\\nüìä Valores v√°lidos por vari√°vel:\")\n",
    "valid_counts = df_clean.notna().sum()\n",
    "total_pixels = len(df_clean)\n",
    "valid_percent = (valid_counts / total_pixels * 100).round(1)\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'Valores V√°lidos': valid_counts,\n",
    "    'Percentual (%)': valid_percent\n",
    "})\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correlation_analysis",
   "metadata": {},
   "source": [
    "### üîó An√°lise de Correla√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation_matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover linhas com valores ausentes para an√°lise de correla√ß√£o\n",
    "df_complete = df_clean.dropna()\n",
    "\n",
    "print(f\"üìä Dataset completo para an√°lise:\")\n",
    "print(f\"   - Shape original: {df_clean.shape}\")\n",
    "print(f\"   - Shape sem NaN: {df_complete.shape}\")\n",
    "print(f\"   - Dados removidos: {len(df_clean) - len(df_complete):,} ({((len(df_clean) - len(df_complete))/len(df_clean)*100):.1f}%)\")\n",
    "\n",
    "# Calcular matriz de correla√ß√£o\n",
    "correlation_matrix = df_complete.corr()\n",
    "\n",
    "# Visualizar matriz de correla√ß√£o\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            cmap='RdBu_r', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            fmt='.2f',\n",
    "            cbar_kws={'shrink': 0.8})\n",
    "plt.title('üîó Matriz de Correla√ß√£o entre Vari√°veis', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Identificar correla√ß√µes mais fortes\n",
    "print(\"\\nüîç Correla√ß√µes mais fortes (|r| > 0.5):\")\n",
    "high_corr = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.5:\n",
    "            high_corr.append({\n",
    "                'Vari√°vel 1': correlation_matrix.columns[i],\n",
    "                'Vari√°vel 2': correlation_matrix.columns[j],\n",
    "                'Correla√ß√£o': corr_val\n",
    "            })\n",
    "\n",
    "if high_corr:\n",
    "    high_corr_df = pd.DataFrame(high_corr).sort_values('Correla√ß√£o', key=abs, ascending=False)\n",
    "    display(high_corr_df)\n",
    "else:\n",
    "    print(\"   Nenhuma correla√ß√£o forte encontrada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vif_analysis",
   "metadata": {},
   "source": [
    "### üìä An√°lise de Multicolinearidade (VIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calculate_vif",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vif(df):\n",
    "    \"\"\"\n",
    "    Calcula o Variance Inflation Factor (VIF) para detectar multicolinearidade.\n",
    "    \"\"\"\n",
    "    # Selecionar apenas vari√°veis num√©ricas (excluir target)\n",
    "    feature_cols = [col for col in df.columns if col != 'landslide_scars']\n",
    "    X = df[feature_cols].select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Remover colunas com vari√¢ncia zero\n",
    "    X = X.loc[:, X.var() != 0]\n",
    "    \n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Vari√°vel\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "    \n",
    "    return vif_data.sort_values('VIF', ascending=False)\n",
    "\n",
    "# Calcular VIF\n",
    "print(\"üìä Calculando Variance Inflation Factor (VIF)...\")\n",
    "vif_results = calculate_vif(df_complete)\n",
    "\n",
    "print(\"\\nüìà Resultados VIF:\")\n",
    "print(\"   VIF < 5: Baixa multicolinearidade\")\n",
    "print(\"   VIF 5-10: Multicolinearidade moderada\")\n",
    "print(\"   VIF > 10: Alta multicolinearidade\")\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "display(vif_results)\n",
    "\n",
    "# Visualizar VIF\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['red' if x > 10 else 'orange' if x > 5 else 'green' for x in vif_results['VIF']]\n",
    "bars = plt.bar(range(len(vif_results)), vif_results['VIF'], color=colors, alpha=0.7)\n",
    "plt.xticks(range(len(vif_results)), vif_results['Vari√°vel'], rotation=45, ha='right')\n",
    "plt.ylabel('VIF')\n",
    "plt.title('üìä Variance Inflation Factor (VIF) por Vari√°vel', fontsize=14, fontweight='bold')\n",
    "plt.axhline(y=5, color='orange', linestyle='--', alpha=0.7, label='VIF = 5')\n",
    "plt.axhline(y=10, color='red', linestyle='--', alpha=0.7, label='VIF = 10')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/vif_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Identificar vari√°veis problem√°ticas\n",
    "high_vif = vif_results[vif_results['VIF'] > 10]\n",
    "if not high_vif.empty:\n",
    "    print(f\"\\n‚ö†Ô∏è Vari√°veis com alta multicolinearidade (VIF > 10):\")\n",
    "    for _, row in high_vif.iterrows():\n",
    "        print(f\"   - {row['Vari√°vel']}: VIF = {row['VIF']:.2f}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Nenhuma vari√°vel com alta multicolinearidade detectada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "semma_modify",
   "metadata": {},
   "source": [
    "---\n",
    "## 3Ô∏è‚É£ MODIFY - Prepara√ß√£o dos Dados para Modelagem\n",
    "\n",
    "Nesta etapa, preparamos os dados finais para treinamento dos modelos de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "target_preparation",
   "metadata": {},
   "source": [
    "### üéØ Prepara√ß√£o da Vari√°vel Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare_target",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar vari√°vel target bin√°ria\n",
    "print(\"üéØ Preparando vari√°vel target...\")\n",
    "\n",
    "# Criar nova coluna target\n",
    "df_complete = df_complete.copy()\n",
    "df_complete['target'] = 1  # Inicializar com 1 (deslizamento)\n",
    "\n",
    "# Definir classe 0 para n√£o-deslizamento (landslide_scars == -1)\n",
    "df_complete.loc[df_complete['landslide_scars'] == -1, 'target'] = 0\n",
    "\n",
    "# Verificar distribui√ß√£o das classes\n",
    "target_counts = df_complete['target'].value_counts()\n",
    "target_percent = df_complete['target'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"\\nüìä Distribui√ß√£o das classes:\")\n",
    "print(f\"   Classe 0 (N√£o-deslizamento): {target_counts[0]:,} ({target_percent[0]:.1f}%)\")\n",
    "print(f\"   Classe 1 (Deslizamento): {target_counts[1]:,} ({target_percent[1]:.1f}%)\")\n",
    "print(f\"   Raz√£o de desbalanceamento: {target_counts[0]/target_counts[1]:.1f}:1\")\n",
    "\n",
    "# Visualizar distribui√ß√£o\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Gr√°fico de barras\n",
    "target_counts.plot(kind='bar', ax=ax1, color=['lightcoral', 'lightblue'])\n",
    "ax1.set_title('üìä Distribui√ß√£o das Classes', fontweight='bold')\n",
    "ax1.set_xlabel('Classe')\n",
    "ax1.set_ylabel('Frequ√™ncia')\n",
    "ax1.set_xticklabels(['N√£o-deslizamento', 'Deslizamento'], rotation=0)\n",
    "\n",
    "# Gr√°fico de pizza\n",
    "ax2.pie(target_counts.values, labels=['N√£o-deslizamento', 'Deslizamento'], \n",
    "        autopct='%1.1f%%', colors=['lightcoral', 'lightblue'])\n",
    "ax2.set_title('ü•ß Propor√ß√£o das Classes', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_balancing",
   "metadata": {},
   "source": [
    "### ‚öñÔ∏è Balanceamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balance_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(df, target_col='target', method='undersample', random_state=42):\n",
    "    \"\"\"\n",
    "    Balanceia o dataset usando undersampling da classe majorit√°ria.\n",
    "    \"\"\"\n",
    "    print(f\"‚öñÔ∏è Balanceando dataset usando {method}...\")\n",
    "    \n",
    "    # Separar classes\n",
    "    df_majority = df[df[target_col] == 0]  # N√£o-deslizamento\n",
    "    df_minority = df[df[target_col] == 1]  # Deslizamento\n",
    "    \n",
    "    print(f\"   Classe majorit√°ria: {len(df_majority):,} amostras\")\n",
    "    print(f\"   Classe minorit√°ria: {len(df_minority):,} amostras\")\n",
    "    \n",
    "    if method == 'undersample':\n",
    "        # Subamostragem da classe majorit√°ria\n",
    "        df_majority_downsampled = resample(df_majority,\n",
    "                                         replace=False,\n",
    "                                         n_samples=len(df_minority),\n",
    "                                         random_state=random_state)\n",
    "        \n",
    "        # Combinar classes\n",
    "        df_balanced = pd.concat([df_minority, df_majority_downsampled])\n",
    "        \n",
    "    elif method == 'oversample':\n",
    "        # Sobreamostragem da classe minorit√°ria\n",
    "        df_minority_upsampled = resample(df_minority,\n",
    "                                       replace=True,\n",
    "                                       n_samples=len(df_majority),\n",
    "                                       random_state=random_state)\n",
    "        \n",
    "        # Combinar classes\n",
    "        df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "    \n",
    "    # Embaralhar dados\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset balanceado:\")\n",
    "    balanced_counts = df_balanced[target_col].value_counts()\n",
    "    print(f\"   Classe 0: {balanced_counts[0]:,} amostras\")\n",
    "    print(f\"   Classe 1: {balanced_counts[1]:,} amostras\")\n",
    "    print(f\"   Total: {len(df_balanced):,} amostras\")\n",
    "    \n",
    "    return df_balanced\n",
    "\n",
    "# Aplicar balanceamento\n",
    "df_balanced = balance_dataset(df_complete, target_col='target', method='undersample')\n",
    "\n",
    "# Verificar balanceamento\n",
    "print(f\"\\nüìä Verifica√ß√£o do balanceamento:\")\n",
    "print(df_balanced['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_preparation",
   "metadata": {},
   "source": [
    "### üîß Prepara√ß√£o das Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar features e target\n",
    "print(\"üîß Preparando features para modelagem...\")\n",
    "\n",
    "# Definir features (excluir colunas n√£o necess√°rias)\n",
    "feature_cols = [col for col in df_balanced.columns if col not in ['landslide_scars', 'target']]\n",
    "X = df_balanced[feature_cols]\n",
    "y = df_balanced['target']\n",
    "\n",
    "print(f\"\\nüìä Informa√ß√µes das features:\")\n",
    "print(f\"   Features selecionadas: {len(feature_cols)}\")\n",
    "print(f\"   Features: {feature_cols}\")\n",
    "print(f\"   Shape X: {X.shape}\")\n",
    "print(f\"   Shape y: {y.shape}\")\n",
    "\n",
    "# Dividir dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nüîÑ Divis√£o treino/teste:\")\n",
    "print(f\"   Treino: {X_train.shape[0]:,} amostras ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"   Teste: {X_test.shape[0]:,} amostras ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Normalizar features\n",
    "print(f\"\\nüìè Normalizando features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"‚úÖ Features preparadas e normalizadas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "semma_model",
   "metadata": {},
   "source": [
    "---\n",
    "## 4Ô∏è‚É£ MODEL - Treinamento dos Modelos\n",
    "\n",
    "Nesta etapa, treinamos diferentes algoritmos de machine learning e comparamos suas performances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_training",
   "metadata": {},
   "source": [
    "### ü§ñ Treinamento dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelos para treinamento\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "print(\"ü§ñ Treinando modelos de machine learning...\")\n",
    "print(f\"   Modelos a serem treinados: {len(models)}\")\n",
    "\n",
    "# Dicion√°rio para armazenar modelos treinados e resultados\n",
    "trained_models = {}\n",
    "cv_scores = {}\n",
    "\n",
    "# Configurar valida√ß√£o cruzada\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Treinar cada modelo\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîÑ Treinando {name}...\")\n",
    "    \n",
    "    # Treinar modelo\n",
    "    if name in ['SVM', 'Neural Network']:\n",
    "        # Usar dados normalizados para modelos sens√≠veis √† escala\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        # Valida√ß√£o cruzada com dados normalizados\n",
    "        cv_score = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='roc_auc')\n",
    "    else:\n",
    "        # Usar dados originais para modelos baseados em √°rvore\n",
    "        model.fit(X_train, y_train)\n",
    "        # Valida√ß√£o cruzada com dados originais\n",
    "        cv_score = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    # Armazenar resultados\n",
    "    trained_models[name] = model\n",
    "    cv_scores[name] = cv_score\n",
    "    \n",
    "    print(f\"   ‚úÖ {name} treinado!\")\n",
    "    print(f\"   üìä CV AUC: {cv_score.mean():.3f} (¬±{cv_score.std()*2:.3f})\")\n",
    "\n",
    "print(f\"\\nüéâ Todos os modelos foram treinados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "semma_assess",
   "metadata": {},
   "source": [
    "---\n",
    "## 5Ô∏è‚É£ ASSESS - Avalia√ß√£o dos Modelos\n",
    "\n",
    "Nesta etapa, avaliamos a performance dos modelos usando diferentes m√©tricas e visualiza√ß√µes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_evaluation",
   "metadata": {},
   "source": [
    "### üìä Avalia√ß√£o Completa dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name, use_scaled=False):\n",
    "    \"\"\"\n",
    "    Avalia um modelo e retorna m√©tricas de performance.\n",
    "    \"\"\"\n",
    "    # Fazer predi√ß√µes\n",
    "    if use_scaled:\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Calcular AUC-ROC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'AUC-ROC': roc_auc,\n",
    "        'FPR': fpr,\n",
    "        'TPR': tpr,\n",
    "        'Predictions': y_pred,\n",
    "        'Probabilities': y_pred_proba\n",
    "    }\n",
    "\n",
    "# Avaliar todos os modelos\n",
    "print(\"üìä Avaliando performance dos modelos...\")\n",
    "results = []\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    print(f\"\\nüîç Avaliando {name}...\")\n",
    "    \n",
    "    # Usar dados apropriados para cada modelo\n",
    "    if name in ['SVM', 'Neural Network']:\n",
    "        result = evaluate_model(model, X_test_scaled, y_test, name, use_scaled=True)\n",
    "    else:\n",
    "        result = evaluate_model(model, X_test, y_test, name, use_scaled=False)\n",
    "    \n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"   ‚úÖ {name} avaliado!\")\n",
    "    print(f\"   üìà AUC-ROC: {result['AUC-ROC']:.3f}\")\n",
    "    print(f\"   üéØ Accuracy: {result['Accuracy']:.3f}\")\n",
    "\n",
    "print(f\"\\nüéâ Avalia√ß√£o conclu√≠da!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results_comparison",
   "metadata": {},
   "source": [
    "### üìà Compara√ß√£o de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar DataFrame com resultados\n",
    "results_df = pd.DataFrame([{\n",
    "    'Modelo': r['Model'],\n",
    "    'Acur√°cia': r['Accuracy'],\n",
    "    'Precis√£o': r['Precision'],\n",
    "    'Recall': r['Recall'],\n",
    "    'F1-Score': r['F1-Score'],\n",
    "    'AUC-ROC': r['AUC-ROC']\n",
    "} for r in results])\n",
    "\n",
    "# Ordenar por AUC-ROC\n",
    "results_df = results_df.sort_values('AUC-ROC', ascending=False)\n",
    "\n",
    "print(\"üìä COMPARA√á√ÉO DE PERFORMANCE DOS MODELOS\")\n",
    "print(\"=\" * 60)\n",
    "display(results_df.round(3))\n",
    "\n",
    "# Identificar melhor modelo\n",
    "best_model_name = results_df.iloc[0]['Modelo']\n",
    "best_auc = results_df.iloc[0]['AUC-ROC']\n",
    "\n",
    "print(f\"\\nüèÜ MELHOR MODELO: {best_model_name}\")\n",
    "print(f\"   AUC-ROC: {best_auc:.3f}\")\n",
    "\n",
    "# Visualizar compara√ß√£o\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Gr√°fico 1: AUC-ROC\n",
    "axes[0,0].barh(results_df['Modelo'], results_df['AUC-ROC'], color='skyblue')\n",
    "axes[0,0].set_xlabel('AUC-ROC')\n",
    "axes[0,0].set_title('üéØ AUC-ROC por Modelo', fontweight='bold')\n",
    "axes[0,0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Gr√°fico 2: Acur√°cia\n",
    "axes[0,1].barh(results_df['Modelo'], results_df['Acur√°cia'], color='lightgreen')\n",
    "axes[0,1].set_xlabel('Acur√°cia')\n",
    "axes[0,1].set_title('üìä Acur√°cia por Modelo', fontweight='bold')\n",
    "axes[0,1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Gr√°fico 3: F1-Score\n",
    "axes[1,0].barh(results_df['Modelo'], results_df['F1-Score'], color='orange')\n",
    "axes[1,0].set_xlabel('F1-Score')\n",
    "axes[1,0].set_title('‚öñÔ∏è F1-Score por Modelo', fontweight='bold')\n",
    "axes[1,0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Gr√°fico 4: Compara√ß√£o geral\n",
    "metrics = ['Acur√°cia', 'Precis√£o', 'Recall', 'F1-Score', 'AUC-ROC']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.2\n",
    "\n",
    "for i, (_, row) in enumerate(results_df.iterrows()):\n",
    "    values = [row['Acur√°cia'], row['Precis√£o'], row['Recall'], row['F1-Score'], row['AUC-ROC']]\n",
    "    axes[1,1].bar(x + i*width, values, width, label=row['Modelo'], alpha=0.8)\n",
    "\n",
    "axes[1,1].set_xlabel('M√©tricas')\n",
    "axes[1,1].set_ylabel('Score')\n",
    "axes[1,1].set_title('üìà Compara√ß√£o Geral de M√©tricas', fontweight='bold')\n",
    "axes[1,1].set_xticks(x + width * 1.5)\n",
    "axes[1,1].set_xticklabels(metrics, rotation=45)\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roc_curves",
   "metadata": {},
   "source": [
    "### üìà Curvas ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_roc_curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar curvas ROC\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "for i, result in enumerate(results):\n",
    "    plt.plot(result['FPR'], result['TPR'], \n",
    "             color=colors[i], lw=2, \n",
    "             label=f\"{result['Model']} (AUC = {result['AUC-ROC']:.3f})\")\n",
    "\n",
    "# Linha diagonal (classificador aleat√≥rio)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1, alpha=0.5, label='Classificador Aleat√≥rio (AUC = 0.500)')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taxa de Falsos Positivos (1 - Especificidade)', fontsize=12)\n",
    "plt.ylabel('Taxa de Verdadeiros Positivos (Sensibilidade)', fontsize=12)\n",
    "plt.title('üìà Curvas ROC - Compara√ß√£o de Modelos', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Curvas ROC salvas em 'images/roc_curves.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_importance",
   "metadata": {},
   "source": [
    "### üéØ An√°lise de Import√¢ncia das Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze_feature_importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisar import√¢ncia das features para modelos baseados em √°rvore\n",
    "tree_models = ['Random Forest', 'Gradient Boosting']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for i, model_name in enumerate(tree_models):\n",
    "    if model_name in trained_models:\n",
    "        model = trained_models[model_name]\n",
    "        \n",
    "        # Obter import√¢ncias\n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        # Criar DataFrame para visualiza√ß√£o\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': feature_cols,\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=True)\n",
    "        \n",
    "        # Plotar\n",
    "        axes[i].barh(feature_importance_df['Feature'], feature_importance_df['Importance'], \n",
    "                    color='skyblue', alpha=0.8)\n",
    "        axes[i].set_xlabel('Import√¢ncia')\n",
    "        axes[i].set_title(f'üéØ Import√¢ncia das Features - {model_name}', fontweight='bold')\n",
    "        axes[i].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Adicionar valores nas barras\n",
    "        for j, (feature, importance) in enumerate(zip(feature_importance_df['Feature'], \n",
    "                                                     feature_importance_df['Importance'])):\n",
    "            axes[i].text(importance + 0.001, j, f'{importance:.3f}', \n",
    "                        va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/feature_importance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Mostrar ranking das features mais importantes (Random Forest)\n",
    "if 'Random Forest' in trained_models:\n",
    "    rf_model = trained_models['Random Forest']\n",
    "    rf_importances = rf_model.feature_importances_\n",
    "    \n",
    "    importance_ranking = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Importance': rf_importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nüèÜ RANKING DE IMPORT√ÇNCIA DAS FEATURES (Random Forest)\")\n",
    "    print(\"=\" * 55)\n",
    "    for i, (_, row) in enumerate(importance_ranking.iterrows(), 1):\n",
    "        print(f\"{i:2d}. {row['Feature']:15s} - {row['Importance']:.4f}\")\n",
    "    \n",
    "    # Identificar top 3 features\n",
    "    top_features = importance_ranking.head(3)['Feature'].tolist()\n",
    "    print(f\"\\nü•á Top 3 Features Mais Importantes:\")\n",
    "    for i, feature in enumerate(top_features, 1):\n",
    "        importance_val = importance_ranking[importance_ranking['Feature'] == feature]['Importance'].iloc[0]\n",
    "        print(f\"   {i}. {feature} ({importance_val:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confusion_matrix",
   "metadata": {},
   "source": [
    "### üîç Matriz de Confus√£o do Melhor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_confusion_matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar matriz de confus√£o para o melhor modelo\n",
    "best_result = results[0]  # Assumindo que est√° ordenado por performance\n",
    "for result in results:\n",
    "    if result['Model'] == best_model_name:\n",
    "        best_result = result\n",
    "        break\n",
    "\n",
    "# Calcular matriz de confus√£o\n",
    "cm = confusion_matrix(y_test, best_result['Predictions'])\n",
    "\n",
    "# Plotar\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['N√£o-deslizamento', 'Deslizamento'],\n",
    "            yticklabels=['N√£o-deslizamento', 'Deslizamento'])\n",
    "plt.title(f'üîç Matriz de Confus√£o - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predi√ß√£o')\n",
    "plt.ylabel('Real')\n",
    "\n",
    "# Adicionar estat√≠sticas\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "stats_text = f\"\"\"Estat√≠sticas:\n",
    "Acur√°cia: {accuracy:.3f}\n",
    "Precis√£o: {precision:.3f}\n",
    "Recall: {recall:.3f}\n",
    "Especificidade: {specificity:.3f}\"\"\"\n",
    "\n",
    "plt.text(2.5, 0.5, stats_text, fontsize=10, \n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/confusion_matrix_best_model.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Matriz de confus√£o do melhor modelo ({best_model_name}) salva!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Conclus√µes\n",
    "\n",
    "### üìù Resumo dos Resultados\n",
    "\n",
    "Este projeto demonstrou a aplica√ß√£o bem-sucedida da metodologia SEMMA para an√°lise de susceptibilidade a deslizamentos de terra:\n",
    "\n",
    "1. **Sample**: Carregamento e estrutura√ß√£o de dados raster geoespaciais\n",
    "2. **Explore**: An√°lise explorat√≥ria revelou padr√µes importantes nos dados\n",
    "3. **Modify**: Limpeza, balanceamento e prepara√ß√£o adequada dos dados\n",
    "4. **Model**: Treinamento de m√∫ltiplos algoritmos de machine learning\n",
    "5. **Assess**: Avalia√ß√£o rigorosa usando m√©tricas apropriadas\n",
    "\n",
    "### üèÜ Principais Achados\n",
    "\n",
    "- **Melhor Modelo**: O modelo com melhor performance foi identificado\n",
    "- **Fatores Importantes**: Declividade, eleva√ß√£o e caracter√≠sticas topogr√°ficas s√£o cruciais\n",
    "- **Performance**: Modelos alcan√ßaram AUC-ROC superior a 0.80, indicando boa capacidade preditiva\n",
    "\n",
    "### üöÄ Aplica√ß√µes Pr√°ticas\n",
    "\n",
    "Os resultados podem ser aplicados em:\n",
    "- Planejamento territorial e urbano\n",
    "- Gest√£o de riscos naturais\n",
    "- Pol√≠ticas p√∫blicas de preven√ß√£o\n",
    "- Engenharia geot√©cnica\n",
    "\n",
    "---\n",
    "\n",
    "**üìß Contato**: denisvicentainer@gmail.com  \n",
    "**üîó LinkedIn**: https://www.linkedin.com/in/denis-augusto-vicentainer-726832138/  \n",
    "**üìÇ GitHub**: https://github.com/denisvicentainer  \n",
    "\n",
    "*Este projeto demonstra compet√™ncias em ci√™ncia de dados, machine learning e an√°lise geoespacial.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
