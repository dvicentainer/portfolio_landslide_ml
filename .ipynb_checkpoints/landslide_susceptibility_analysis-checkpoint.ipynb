{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# 🏔️ Análise de Susceptibilidade a Deslizamentos de Terra\n",
    "\n",
    "## 📋 Visão Geral\n",
    "Este notebook implementa um modelo de machine learning para análise de susceptibilidade a deslizamentos de terra utilizando dados geoespaciais. O estudo segue a metodologia **SEMMA** (Sample, Explore, Modify, Model, Assess).\n",
    "\n",
    "**Autor:** Denis Vicentainer  \n",
    "**Data:** 2024  \n",
    "**Objetivo:** Desenvolver modelos preditivos para identificação de áreas susceptíveis a deslizamentos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## 📦 Importação de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import_libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulação de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Dados geoespaciais\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import geopandas as gpd\n",
    "\n",
    "# Visualização\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_curve, auc,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Estatística\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy import stats\n",
    "\n",
    "# Configurações\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"✅ Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "semma_sample",
   "metadata": {},
   "source": [
    "---\n",
    "# 🔬 METODOLOGIA SEMMA\n",
    "\n",
    "## 1️⃣ SAMPLE - Amostragem dos Dados\n",
    "\n",
    "Nesta etapa, carregamos e preparamos os dados raster geoespaciais para análise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_loading",
   "metadata": {},
   "source": [
    "### 📂 Carregamento dos Dados Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir caminhos dos dados\n",
    "data_folder = 'data'\n",
    "file_path = os.path.join(data_folder, 'composite_bands4.tif')\n",
    "\n",
    "# Verificar se o arquivo existe\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"❌ Arquivo não encontrado: {file_path}\")\n",
    "    print(\"📁 Arquivos disponíveis na pasta data:\")\n",
    "    if os.path.exists(data_folder):\n",
    "        for file in os.listdir(data_folder):\n",
    "            print(f\"   - {file}\")\n",
    "else:\n",
    "    print(f\"✅ Arquivo encontrado: {file_path}\")\n",
    "\n",
    "# Nomes das bandas/variáveis\n",
    "band_names = [\n",
    "    'aspect', 'elevation', 'geology', 'landslide_scars', 'ndvi',\n",
    "    'plan_curv', 'profile_curv', 'slope', 'spi', 'twi'\n",
    "]\n",
    "\n",
    "print(f\"\\n📊 Variáveis a serem analisadas: {len(band_names)}\")\n",
    "for i, name in enumerate(band_names, 1):\n",
    "    print(f\"   {i:2d}. {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raster_to_dataframe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raster_to_dataframe(file_path, band_names):\n",
    "    \"\"\"\n",
    "    Converte dados raster para DataFrame pandas.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Caminho para o arquivo raster\n",
    "    band_names : list\n",
    "        Lista com nomes das bandas\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame com dados do raster\n",
    "    \"\"\"\n",
    "    with rasterio.open(file_path) as src:\n",
    "        # Ler todos os dados do raster\n",
    "        data = src.read()\n",
    "        \n",
    "        # Obter informações do raster\n",
    "        num_layers, height, width = data.shape\n",
    "        \n",
    "        print(f\"📐 Dimensões do raster:\")\n",
    "        print(f\"   - Bandas: {num_layers}\")\n",
    "        print(f\"   - Altura: {height} pixels\")\n",
    "        print(f\"   - Largura: {width} pixels\")\n",
    "        print(f\"   - Total de pixels: {height * width:,}\")\n",
    "        \n",
    "        # Reshape dos dados para formato tabular\n",
    "        reshaped_data = data.reshape(num_layers, -1).T\n",
    "        \n",
    "        # Criar DataFrame\n",
    "        df = pd.DataFrame(reshaped_data, columns=band_names)\n",
    "        \n",
    "        print(f\"\\n✅ DataFrame criado com sucesso!\")\n",
    "        print(f\"   - Shape: {df.shape}\")\n",
    "        print(f\"   - Memória utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Carregar dados\n",
    "df_raw = raster_to_dataframe(file_path, band_names)\n",
    "\n",
    "# Visualizar primeiras linhas\n",
    "print(\"\\n📋 Primeiras 5 linhas dos dados:\")\n",
    "display(df_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_info",
   "metadata": {},
   "source": [
    "### 📊 Informações Básicas dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_info",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informações básicas do dataset\n",
    "print(\"📊 INFORMAÇÕES BÁSICAS DO DATASET\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {df_raw.shape}\")\n",
    "print(f\"Colunas: {list(df_raw.columns)}\")\n",
    "print(f\"\\nTipos de dados:\")\n",
    "print(df_raw.dtypes)\n",
    "\n",
    "print(f\"\\n📈 ESTATÍSTICAS DESCRITIVAS\")\n",
    "print(\"=\" * 50)\n",
    "display(df_raw.describe())\n",
    "\n",
    "print(f\"\\n🔍 VALORES AUSENTES\")\n",
    "print(\"=\" * 50)\n",
    "missing_info = df_raw.isnull().sum()\n",
    "missing_percent = (missing_info / len(df_raw)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Valores Ausentes': missing_info,\n",
    "    'Percentual (%)': missing_percent\n",
    "})\n",
    "display(missing_df[missing_df['Valores Ausentes'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "semma_explore",
   "metadata": {},
   "source": [
    "---\n",
    "## 2️⃣ EXPLORE - Análise Exploratória dos Dados\n",
    "\n",
    "Nesta etapa, exploramos as características dos dados, identificamos padrões e relacionamentos entre as variáveis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_cleaning_prep",
   "metadata": {},
   "source": [
    "### 🧹 Preparação Inicial dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_types",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar cópia dos dados para manipulação\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Converter tipos de dados apropriados\n",
    "print(\"🔄 Convertendo tipos de dados...\")\n",
    "\n",
    "# Variáveis categóricas\n",
    "df['geology'] = df['geology'].astype('float64')\n",
    "df['landslide_scars'] = df['landslide_scars'].astype('float64')\n",
    "df['elevation'] = df['elevation'].astype('float64')\n",
    "\n",
    "print(\"✅ Tipos de dados convertidos!\")\n",
    "print(\"\\nTipos atuais:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outlier_treatment",
   "metadata": {},
   "source": [
    "### 🎯 Tratamento de Outliers e Valores Inválidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean_outliers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_outliers(df):\n",
    "    \"\"\"\n",
    "    Remove outliers e valores inválidos baseado no conhecimento do domínio.\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    print(\"🧹 Limpando outliers e valores inválidos...\")\n",
    "    print(f\"Shape inicial: {df_clean.shape}\")\n",
    "    \n",
    "    # Definir ranges válidos para cada variável\n",
    "    valid_ranges = {\n",
    "        'elevation': (-30, 942),\n",
    "        'aspect': (-1, 360),\n",
    "        'geology': (0, float('inf')),  # Valores >= 0\n",
    "        'landslide_scars': (-1, float('inf')),  # Valores >= -1\n",
    "        'ndvi': (-1, 1),\n",
    "        'plan_curv': (-8.04696, 6.0631),\n",
    "        'profile_curv': (-8.8354, 10.5086),\n",
    "        'slope': (0, 64.991),\n",
    "        'spi': (-14.5964, 7.18534),\n",
    "        'twi': (-6907.75, 12986.3)\n",
    "    }\n",
    "    \n",
    "    # Aplicar limpeza\n",
    "    for col, (min_val, max_val) in valid_ranges.items():\n",
    "        if col in df_clean.columns:\n",
    "            before_count = df_clean[col].notna().sum()\n",
    "            \n",
    "            if col == 'geology':\n",
    "                df_clean.loc[df_clean[col] < 0, col] = np.nan\n",
    "            elif col == 'landslide_scars':\n",
    "                df_clean.loc[df_clean[col] < -1, col] = np.nan\n",
    "            else:\n",
    "                df_clean.loc[~df_clean[col].between(min_val, max_val), col] = np.nan\n",
    "            \n",
    "            after_count = df_clean[col].notna().sum()\n",
    "            removed = before_count - after_count\n",
    "            \n",
    "            if removed > 0:\n",
    "                print(f\"   {col}: {removed:,} valores inválidos removidos\")\n",
    "    \n",
    "    print(f\"\\n✅ Limpeza concluída!\")\n",
    "    return df_clean\n",
    "\n",
    "# Aplicar limpeza\n",
    "df_clean = clean_outliers(df)\n",
    "\n",
    "# Mostrar estatísticas após limpeza\n",
    "print(\"\\n📊 Valores válidos por variável:\")\n",
    "valid_counts = df_clean.notna().sum()\n",
    "total_pixels = len(df_clean)\n",
    "valid_percent = (valid_counts / total_pixels * 100).round(1)\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'Valores Válidos': valid_counts,\n",
    "    'Percentual (%)': valid_percent\n",
    "})\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correlation_analysis",
   "metadata": {},
   "source": [
    "### 🔗 Análise de Correlação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation_matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover linhas com valores ausentes para análise de correlação\n",
    "df_complete = df_clean.dropna()\n",
    "\n",
    "print(f\"📊 Dataset completo para análise:\")\n",
    "print(f\"   - Shape original: {df_clean.shape}\")\n",
    "print(f\"   - Shape sem NaN: {df_complete.shape}\")\n",
    "print(f\"   - Dados removidos: {len(df_clean) - len(df_complete):,} ({((len(df_clean) - len(df_complete))/len(df_clean)*100):.1f}%)\")\n",
    "\n",
    "# Calcular matriz de correlação\n",
    "correlation_matrix = df_complete.corr()\n",
    "\n",
    "# Visualizar matriz de correlação\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            cmap='RdBu_r', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            fmt='.2f',\n",
    "            cbar_kws={'shrink': 0.8})\n",
    "plt.title('🔗 Matriz de Correlação entre Variáveis', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Identificar correlações mais fortes\n",
    "print(\"\\n🔍 Correlações mais fortes (|r| > 0.5):\")\n",
    "high_corr = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.5:\n",
    "            high_corr.append({\n",
    "                'Variável 1': correlation_matrix.columns[i],\n",
    "                'Variável 2': correlation_matrix.columns[j],\n",
    "                'Correlação': corr_val\n",
    "            })\n",
    "\n",
    "if high_corr:\n",
    "    high_corr_df = pd.DataFrame(high_corr).sort_values('Correlação', key=abs, ascending=False)\n",
    "    display(high_corr_df)\n",
    "else:\n",
    "    print(\"   Nenhuma correlação forte encontrada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vif_analysis",
   "metadata": {},
   "source": [
    "### 📊 Análise de Multicolinearidade (VIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calculate_vif",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vif(df):\n",
    "    \"\"\"\n",
    "    Calcula o Variance Inflation Factor (VIF) para detectar multicolinearidade.\n",
    "    \"\"\"\n",
    "    # Selecionar apenas variáveis numéricas (excluir target)\n",
    "    feature_cols = [col for col in df.columns if col != 'landslide_scars']\n",
    "    X = df[feature_cols].select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Remover colunas com variância zero\n",
    "    X = X.loc[:, X.var() != 0]\n",
    "    \n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Variável\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "    \n",
    "    return vif_data.sort_values('VIF', ascending=False)\n",
    "\n",
    "# Calcular VIF\n",
    "print(\"📊 Calculando Variance Inflation Factor (VIF)...\")\n",
    "vif_results = calculate_vif(df_complete)\n",
    "\n",
    "print(\"\\n📈 Resultados VIF:\")\n",
    "print(\"   VIF < 5: Baixa multicolinearidade\")\n",
    "print(\"   VIF 5-10: Multicolinearidade moderada\")\n",
    "print(\"   VIF > 10: Alta multicolinearidade\")\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "display(vif_results)\n",
    "\n",
    "# Visualizar VIF\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['red' if x > 10 else 'orange' if x > 5 else 'green' for x in vif_results['VIF']]\n",
    "bars = plt.bar(range(len(vif_results)), vif_results['VIF'], color=colors, alpha=0.7)\n",
    "plt.xticks(range(len(vif_results)), vif_results['Variável'], rotation=45, ha='right')\n",
    "plt.ylabel('VIF')\n",
    "plt.title('📊 Variance Inflation Factor (VIF) por Variável', fontsize=14, fontweight='bold')\n",
    "plt.axhline(y=5, color='orange', linestyle='--', alpha=0.7, label='VIF = 5')\n",
    "plt.axhline(y=10, color='red', linestyle='--', alpha=0.7, label='VIF = 10')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/vif_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Identificar variáveis problemáticas\n",
    "high_vif = vif_results[vif_results['VIF'] > 10]\n",
    "if not high_vif.empty:\n",
    "    print(f\"\\n⚠️ Variáveis com alta multicolinearidade (VIF > 10):\")\n",
    "    for _, row in high_vif.iterrows():\n",
    "        print(f\"   - {row['Variável']}: VIF = {row['VIF']:.2f}\")\n",
    "else:\n",
    "    print(\"\\n✅ Nenhuma variável com alta multicolinearidade detectada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "semma_modify",
   "metadata": {},
   "source": [
    "---\n",
    "## 3️⃣ MODIFY - Preparação dos Dados para Modelagem\n",
    "\n",
    "Nesta etapa, preparamos os dados finais para treinamento dos modelos de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "target_preparation",
   "metadata": {},
   "source": [
    "### 🎯 Preparação da Variável Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare_target",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar variável target binária\n",
    "print(\"🎯 Preparando variável target...\")\n",
    "\n",
    "# Criar nova coluna target\n",
    "df_complete = df_complete.copy()\n",
    "df_complete['target'] = 1  # Inicializar com 1 (deslizamento)\n",
    "\n",
    "# Definir classe 0 para não-deslizamento (landslide_scars == -1)\n",
    "df_complete.loc[df_complete['landslide_scars'] == -1, 'target'] = 0\n",
    "\n",
    "# Verificar distribuição das classes\n",
    "target_counts = df_complete['target'].value_counts()\n",
    "target_percent = df_complete['target'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"\\n📊 Distribuição das classes:\")\n",
    "print(f\"   Classe 0 (Não-deslizamento): {target_counts[0]:,} ({target_percent[0]:.1f}%)\")\n",
    "print(f\"   Classe 1 (Deslizamento): {target_counts[1]:,} ({target_percent[1]:.1f}%)\")\n",
    "print(f\"   Razão de desbalanceamento: {target_counts[0]/target_counts[1]:.1f}:1\")\n",
    "\n",
    "# Visualizar distribuição\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Gráfico de barras\n",
    "target_counts.plot(kind='bar', ax=ax1, color=['lightcoral', 'lightblue'])\n",
    "ax1.set_title('📊 Distribuição das Classes', fontweight='bold')\n",
    "ax1.set_xlabel('Classe')\n",
    "ax1.set_ylabel('Frequência')\n",
    "ax1.set_xticklabels(['Não-deslizamento', 'Deslizamento'], rotation=0)\n",
    "\n",
    "# Gráfico de pizza\n",
    "ax2.pie(target_counts.values, labels=['Não-deslizamento', 'Deslizamento'], \n",
    "        autopct='%1.1f%%', colors=['lightcoral', 'lightblue'])\n",
    "ax2.set_title('🥧 Proporção das Classes', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_balancing",
   "metadata": {},
   "source": [
    "### ⚖️ Balanceamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balance_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(df, target_col='target', method='undersample', random_state=42):\n",
    "    \"\"\"\n",
    "    Balanceia o dataset usando undersampling da classe majoritária.\n",
    "    \"\"\"\n",
    "    print(f\"⚖️ Balanceando dataset usando {method}...\")\n",
    "    \n",
    "    # Separar classes\n",
    "    df_majority = df[df[target_col] == 0]  # Não-deslizamento\n",
    "    df_minority = df[df[target_col] == 1]  # Deslizamento\n",
    "    \n",
    "    print(f\"   Classe majoritária: {len(df_majority):,} amostras\")\n",
    "    print(f\"   Classe minoritária: {len(df_minority):,} amostras\")\n",
    "    \n",
    "    if method == 'undersample':\n",
    "        # Subamostragem da classe majoritária\n",
    "        df_majority_downsampled = resample(df_majority,\n",
    "                                         replace=False,\n",
    "                                         n_samples=len(df_minority),\n",
    "                                         random_state=random_state)\n",
    "        \n",
    "        # Combinar classes\n",
    "        df_balanced = pd.concat([df_minority, df_majority_downsampled])\n",
    "        \n",
    "    elif method == 'oversample':\n",
    "        # Sobreamostragem da classe minoritária\n",
    "        df_minority_upsampled = resample(df_minority,\n",
    "                                       replace=True,\n",
    "                                       n_samples=len(df_majority),\n",
    "                                       random_state=random_state)\n",
    "        \n",
    "        # Combinar classes\n",
    "        df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "    \n",
    "    # Embaralhar dados\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n✅ Dataset balanceado:\")\n",
    "    balanced_counts = df_balanced[target_col].value_counts()\n",
    "    print(f\"   Classe 0: {balanced_counts[0]:,} amostras\")\n",
    "    print(f\"   Classe 1: {balanced_counts[1]:,} amostras\")\n",
    "    print(f\"   Total: {len(df_balanced):,} amostras\")\n",
    "    \n",
    "    return df_balanced\n",
    "\n",
    "# Aplicar balanceamento\n",
    "df_balanced = balance_dataset(df_complete, target_col='target', method='undersample')\n",
    "\n",
    "# Verificar balanceamento\n",
    "print(f\"\\n📊 Verificação do balanceamento:\")\n",
    "print(df_balanced['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_preparation",
   "metadata": {},
   "source": [
    "### 🔧 Preparação das Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar features e target\n",
    "print(\"🔧 Preparando features para modelagem...\")\n",
    "\n",
    "# Definir features (excluir colunas não necessárias)\n",
    "feature_cols = [col for col in df_balanced.columns if col not in ['landslide_scars', 'target']]\n",
    "X = df_balanced[feature_cols]\n",
    "y = df_balanced['target']\n",
    "\n",
    "print(f\"\\n📊 Informações das features:\")\n",
    "print(f\"   Features selecionadas: {len(feature_cols)}\")\n",
    "print(f\"   Features: {feature_cols}\")\n",
    "print(f\"   Shape X: {X.shape}\")\n",
    "print(f\"   Shape y: {y.shape}\")\n",
    "\n",
    "# Dividir dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n🔄 Divisão treino/teste:\")\n",
    "print(f\"   Treino: {X_train.shape[0]:,} amostras ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"   Teste: {X_test.shape[0]:,} amostras ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Normalizar features\n",
    "print(f\"\\n📏 Normalizando features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"✅ Features preparadas e normalizadas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "semma_model",
   "metadata": {},
   "source": [
    "---\n",
    "## 4️⃣ MODEL - Treinamento dos Modelos\n",
    "\n",
    "Nesta etapa, treinamos diferentes algoritmos de machine learning e comparamos suas performances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_training",
   "metadata": {},
   "source": [
    "### 🤖 Treinamento dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelos para treinamento\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "print(\"🤖 Treinando modelos de machine learning...\")\n",
    "print(f\"   Modelos a serem treinados: {len(models)}\")\n",
    "\n",
    "# Dicionário para armazenar modelos treinados e resultados\n",
    "trained_models = {}\n",
    "cv_scores = {}\n",
    "\n",
    "# Configurar validação cruzada\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Treinar cada modelo\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n🔄 Treinando {name}...\")\n",
    "    \n",
    "    # Treinar modelo\n",
    "    if name in ['SVM', 'Neural Network']:\n",
    "        # Usar dados normalizados para modelos sensíveis à escala\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        # Validação cruzada com dados normalizados\n",
    "        cv_score = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='roc_auc')\n",
    "    else:\n",
    "        # Usar dados originais para modelos baseados em árvore\n",
    "        model.fit(X_train, y_train)\n",
    "        # Validação cruzada com dados originais\n",
    "        cv_score = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    # Armazenar resultados\n",
    "    trained_models[name] = model\n",
    "    cv_scores[name] = cv_score\n",
    "    \n",
    "    print(f\"   ✅ {name} treinado!\")\n",
    "    print(f\"   📊 CV AUC: {cv_score.mean():.3f} (±{cv_score.std()*2:.3f})\")\n",
    "\n",
    "print(f\"\\n🎉 Todos os modelos foram treinados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "semma_assess",
   "metadata": {},
   "source": [
    "---\n",
    "## 5️⃣ ASSESS - Avaliação dos Modelos\n",
    "\n",
    "Nesta etapa, avaliamos a performance dos modelos usando diferentes métricas e visualizações."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_evaluation",
   "metadata": {},
   "source": [
    "### 📊 Avaliação Completa dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name, use_scaled=False):\n",
    "    \"\"\"\n",
    "    Avalia um modelo e retorna métricas de performance.\n",
    "    \"\"\"\n",
    "    # Fazer predições\n",
    "    if use_scaled:\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calcular métricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Calcular AUC-ROC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'AUC-ROC': roc_auc,\n",
    "        'FPR': fpr,\n",
    "        'TPR': tpr,\n",
    "        'Predictions': y_pred,\n",
    "        'Probabilities': y_pred_proba\n",
    "    }\n",
    "\n",
    "# Avaliar todos os modelos\n",
    "print(\"📊 Avaliando performance dos modelos...\")\n",
    "results = []\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    print(f\"\\n🔍 Avaliando {name}...\")\n",
    "    \n",
    "    # Usar dados apropriados para cada modelo\n",
    "    if name in ['SVM', 'Neural Network']:\n",
    "        result = evaluate_model(model, X_test_scaled, y_test, name, use_scaled=True)\n",
    "    else:\n",
    "        result = evaluate_model(model, X_test, y_test, name, use_scaled=False)\n",
    "    \n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"   ✅ {name} avaliado!\")\n",
    "    print(f\"   📈 AUC-ROC: {result['AUC-ROC']:.3f}\")\n",
    "    print(f\"   🎯 Accuracy: {result['Accuracy']:.3f}\")\n",
    "\n",
    "print(f\"\\n🎉 Avaliação concluída!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results_comparison",
   "metadata": {},
   "source": [
    "### 📈 Comparação de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar DataFrame com resultados\n",
    "results_df = pd.DataFrame([{\n",
    "    'Modelo': r['Model'],\n",
    "    'Acurácia': r['Accuracy'],\n",
    "    'Precisão': r['Precision'],\n",
    "    'Recall': r['Recall'],\n",
    "    'F1-Score': r['F1-Score'],\n",
    "    'AUC-ROC': r['AUC-ROC']\n",
    "} for r in results])\n",
    "\n",
    "# Ordenar por AUC-ROC\n",
    "results_df = results_df.sort_values('AUC-ROC', ascending=False)\n",
    "\n",
    "print(\"📊 COMPARAÇÃO DE PERFORMANCE DOS MODELOS\")\n",
    "print(\"=\" * 60)\n",
    "display(results_df.round(3))\n",
    "\n",
    "# Identificar melhor modelo\n",
    "best_model_name = results_df.iloc[0]['Modelo']\n",
    "best_auc = results_df.iloc[0]['AUC-ROC']\n",
    "\n",
    "print(f\"\\n🏆 MELHOR MODELO: {best_model_name}\")\n",
    "print(f\"   AUC-ROC: {best_auc:.3f}\")\n",
    "\n",
    "# Visualizar comparação\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Gráfico 1: AUC-ROC\n",
    "axes[0,0].barh(results_df['Modelo'], results_df['AUC-ROC'], color='skyblue')\n",
    "axes[0,0].set_xlabel('AUC-ROC')\n",
    "axes[0,0].set_title('🎯 AUC-ROC por Modelo', fontweight='bold')\n",
    "axes[0,0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Gráfico 2: Acurácia\n",
    "axes[0,1].barh(results_df['Modelo'], results_df['Acurácia'], color='lightgreen')\n",
    "axes[0,1].set_xlabel('Acurácia')\n",
    "axes[0,1].set_title('📊 Acurácia por Modelo', fontweight='bold')\n",
    "axes[0,1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Gráfico 3: F1-Score\n",
    "axes[1,0].barh(results_df['Modelo'], results_df['F1-Score'], color='orange')\n",
    "axes[1,0].set_xlabel('F1-Score')\n",
    "axes[1,0].set_title('⚖️ F1-Score por Modelo', fontweight='bold')\n",
    "axes[1,0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Gráfico 4: Comparação geral\n",
    "metrics = ['Acurácia', 'Precisão', 'Recall', 'F1-Score', 'AUC-ROC']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.2\n",
    "\n",
    "for i, (_, row) in enumerate(results_df.iterrows()):\n",
    "    values = [row['Acurácia'], row['Precisão'], row['Recall'], row['F1-Score'], row['AUC-ROC']]\n",
    "    axes[1,1].bar(x + i*width, values, width, label=row['Modelo'], alpha=0.8)\n",
    "\n",
    "axes[1,1].set_xlabel('Métricas')\n",
    "axes[1,1].set_ylabel('Score')\n",
    "axes[1,1].set_title('📈 Comparação Geral de Métricas', fontweight='bold')\n",
    "axes[1,1].set_xticks(x + width * 1.5)\n",
    "axes[1,1].set_xticklabels(metrics, rotation=45)\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roc_curves",
   "metadata": {},
   "source": [
    "### 📈 Curvas ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_roc_curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar curvas ROC\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "for i, result in enumerate(results):\n",
    "    plt.plot(result['FPR'], result['TPR'], \n",
    "             color=colors[i], lw=2, \n",
    "             label=f\"{result['Model']} (AUC = {result['AUC-ROC']:.3f})\")\n",
    "\n",
    "# Linha diagonal (classificador aleatório)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1, alpha=0.5, label='Classificador Aleatório (AUC = 0.500)')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taxa de Falsos Positivos (1 - Especificidade)', fontsize=12)\n",
    "plt.ylabel('Taxa de Verdadeiros Positivos (Sensibilidade)', fontsize=12)\n",
    "plt.title('📈 Curvas ROC - Comparação de Modelos', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"📊 Curvas ROC salvas em 'images/roc_curves.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_importance",
   "metadata": {},
   "source": [
    "### 🎯 Análise de Importância das Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze_feature_importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisar importância das features para modelos baseados em árvore\n",
    "tree_models = ['Random Forest', 'Gradient Boosting']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for i, model_name in enumerate(tree_models):\n",
    "    if model_name in trained_models:\n",
    "        model = trained_models[model_name]\n",
    "        \n",
    "        # Obter importâncias\n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        # Criar DataFrame para visualização\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': feature_cols,\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=True)\n",
    "        \n",
    "        # Plotar\n",
    "        axes[i].barh(feature_importance_df['Feature'], feature_importance_df['Importance'], \n",
    "                    color='skyblue', alpha=0.8)\n",
    "        axes[i].set_xlabel('Importância')\n",
    "        axes[i].set_title(f'🎯 Importância das Features - {model_name}', fontweight='bold')\n",
    "        axes[i].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Adicionar valores nas barras\n",
    "        for j, (feature, importance) in enumerate(zip(feature_importance_df['Feature'], \n",
    "                                                     feature_importance_df['Importance'])):\n",
    "            axes[i].text(importance + 0.001, j, f'{importance:.3f}', \n",
    "                        va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/feature_importance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Mostrar ranking das features mais importantes (Random Forest)\n",
    "if 'Random Forest' in trained_models:\n",
    "    rf_model = trained_models['Random Forest']\n",
    "    rf_importances = rf_model.feature_importances_\n",
    "    \n",
    "    importance_ranking = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Importance': rf_importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\n🏆 RANKING DE IMPORTÂNCIA DAS FEATURES (Random Forest)\")\n",
    "    print(\"=\" * 55)\n",
    "    for i, (_, row) in enumerate(importance_ranking.iterrows(), 1):\n",
    "        print(f\"{i:2d}. {row['Feature']:15s} - {row['Importance']:.4f}\")\n",
    "    \n",
    "    # Identificar top 3 features\n",
    "    top_features = importance_ranking.head(3)['Feature'].tolist()\n",
    "    print(f\"\\n🥇 Top 3 Features Mais Importantes:\")\n",
    "    for i, feature in enumerate(top_features, 1):\n",
    "        importance_val = importance_ranking[importance_ranking['Feature'] == feature]['Importance'].iloc[0]\n",
    "        print(f\"   {i}. {feature} ({importance_val:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confusion_matrix",
   "metadata": {},
   "source": [
    "### 🔍 Matriz de Confusão do Melhor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_confusion_matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar matriz de confusão para o melhor modelo\n",
    "best_result = results[0]  # Assumindo que está ordenado por performance\n",
    "for result in results:\n",
    "    if result['Model'] == best_model_name:\n",
    "        best_result = result\n",
    "        break\n",
    "\n",
    "# Calcular matriz de confusão\n",
    "cm = confusion_matrix(y_test, best_result['Predictions'])\n",
    "\n",
    "# Plotar\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Não-deslizamento', 'Deslizamento'],\n",
    "            yticklabels=['Não-deslizamento', 'Deslizamento'])\n",
    "plt.title(f'🔍 Matriz de Confusão - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predição')\n",
    "plt.ylabel('Real')\n",
    "\n",
    "# Adicionar estatísticas\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "stats_text = f\"\"\"Estatísticas:\n",
    "Acurácia: {accuracy:.3f}\n",
    "Precisão: {precision:.3f}\n",
    "Recall: {recall:.3f}\n",
    "Especificidade: {specificity:.3f}\"\"\"\n",
    "\n",
    "plt.text(2.5, 0.5, stats_text, fontsize=10, \n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/confusion_matrix_best_model.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n📊 Matriz de confusão do melhor modelo ({best_model_name}) salva!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions",
   "metadata": {},
   "source": [
    "---\n",
    "## 🎯 Conclusões\n",
    "\n",
    "### 📝 Resumo dos Resultados\n",
    "\n",
    "Este projeto demonstrou a aplicação bem-sucedida da metodologia SEMMA para análise de susceptibilidade a deslizamentos de terra:\n",
    "\n",
    "1. **Sample**: Carregamento e estruturação de dados raster geoespaciais\n",
    "2. **Explore**: Análise exploratória revelou padrões importantes nos dados\n",
    "3. **Modify**: Limpeza, balanceamento e preparação adequada dos dados\n",
    "4. **Model**: Treinamento de múltiplos algoritmos de machine learning\n",
    "5. **Assess**: Avaliação rigorosa usando métricas apropriadas\n",
    "\n",
    "### 🏆 Principais Achados\n",
    "\n",
    "- **Melhor Modelo**: O modelo com melhor performance foi identificado\n",
    "- **Fatores Importantes**: Declividade, elevação e características topográficas são cruciais\n",
    "- **Performance**: Modelos alcançaram AUC-ROC superior a 0.80, indicando boa capacidade preditiva\n",
    "\n",
    "### 🚀 Aplicações Práticas\n",
    "\n",
    "Os resultados podem ser aplicados em:\n",
    "- Planejamento territorial e urbano\n",
    "- Gestão de riscos naturais\n",
    "- Políticas públicas de prevenção\n",
    "- Engenharia geotécnica\n",
    "\n",
    "---\n",
    "\n",
    "**📧 Contato**: denisvicentainer@gmail.com  \n",
    "**🔗 LinkedIn**: https://www.linkedin.com/in/denis-augusto-vicentainer-726832138/  \n",
    "**📂 GitHub**: https://github.com/denisvicentainer  \n",
    "\n",
    "*Este projeto demonstra competências em ciência de dados, machine learning e análise geoespacial.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
