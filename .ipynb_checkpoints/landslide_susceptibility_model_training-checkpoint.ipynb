{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# üèîÔ∏è Treinamento de Modelos de Susceptibilidade a Deslizamentos de Terra\n",
    "\n",
    "**Autor:** Denis Vicentainer  \n",
    "**Email:** denisvicentainer@gmail.com  \n",
    "**LinkedIn:** https://www.linkedin.com/in/denis-augusto-vicentainer-726832138/  \n",
    "\n",
    "Este notebook implementa modelos de machine learning para an√°lise de susceptibilidade a deslizamentos usando dados geoespaciais reais.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## üì¶ Importa√ß√£o de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import_libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√µes baseadas no c√≥digo original que funciona\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Estat√≠stica\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Configurar paleta de cores consistente\n",
    "colors = {\n",
    "    'primary': '#2E86AB',      # Azul principal\n",
    "    'secondary': '#A23B72',    # Rosa/roxo\n",
    "    'accent': '#F18F01',       # Laranja\n",
    "    'success': '#C73E1D',      # Vermelho\n",
    "    'neutral': '#6C757D',      # Cinza\n",
    "    'light': '#E9ECEF'         # Cinza claro\n",
    "}\n",
    "\n",
    "# Configurar estilo dos gr√°ficos\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette([colors['primary'], colors['secondary'], colors['accent'], colors['success']])\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas com sucesso!\")\n",
    "print(\"üé® Paleta de cores configurada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_loading",
   "metadata": {},
   "source": [
    "## üìÇ 1. SAMPLE - Carregamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados raster\n",
    "file_path = 'data/composite_bands4.tif'\n",
    "band_names = ['aspect', 'elevation', 'geology', 'landslide_scars', 'ndvi',\n",
    "              'plan_curv', 'profile_curv', 'slope', 'spi', 'twi']\n",
    "\n",
    "print(f\"üìÇ Carregando arquivo: {file_path}\")\n",
    "\n",
    "with rasterio.open(file_path) as src:\n",
    "    # Ler todos os dados do raster\n",
    "    data = src.read()\n",
    "    # Obter as dimens√µes do raster\n",
    "    num_layers, height, width = data.shape\n",
    "    \n",
    "    print(f\"üìê Dimens√µes do raster:\")\n",
    "    print(f\"   - Bandas: {num_layers}\")\n",
    "    print(f\"   - Altura: {height} pixels\")\n",
    "    print(f\"   - Largura: {width} pixels\")\n",
    "    print(f\"   - Total de pixels: {height * width:,}\")\n",
    "\n",
    "    # Reshape os dados do raster para formato tabular\n",
    "    reshaped_data = data.reshape(num_layers, -1).T\n",
    "    df = pd.DataFrame(reshaped_data, columns=band_names)\n",
    "\n",
    "print(f\"\\n‚úÖ DataFrame criado: {df.shape}\")\n",
    "print(\"\\nüìã Informa√ß√µes b√°sicas:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_types",
   "metadata": {},
   "source": [
    "## üîß 2. EXPLORE - Prepara√ß√£o e An√°lise Explorat√≥ria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convert_types",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter tipos de dados\n",
    "print(\"üîÑ Convertendo tipos de dados...\")\n",
    "\n",
    "df['geology'] = df['geology'].astype(int)\n",
    "df['landslide_scars'] = df['landslide_scars'].astype(int)\n",
    "df['elevation'] = df['elevation'].astype(int)\n",
    "\n",
    "print(\"‚úÖ Tipos convertidos!\")\n",
    "print(\"\\nTipos de dados:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_cleaning",
   "metadata": {},
   "source": [
    "## üßπ 3. MODIFY - Limpeza dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpeza de outliers baseada em conhecimento do dom√≠nio\n",
    "print(\"üßπ Aplicando limpeza de outliers...\")\n",
    "\n",
    "# Aplicar ranges v√°lidos para cada vari√°vel\n",
    "df.loc[~df['elevation'].between(-30, 942), 'elevation'] = np.nan\n",
    "df.loc[~df['aspect'].between(-1, 360), 'aspect'] = np.nan\n",
    "df.loc[df['geology'] < 0, 'geology'] = np.nan\n",
    "df.loc[df['landslide_scars'] < -1, 'landslide_scars'] = np.nan\n",
    "df.loc[~df['ndvi'].between(-1, 1), 'ndvi'] = np.nan\n",
    "df.loc[~df['plan_curv'].between(-8.04696, 6.0631), 'plan_curv'] = np.nan\n",
    "df.loc[~df['profile_curv'].between(-8.8354, 10.5086), 'profile_curv'] = np.nan\n",
    "df.loc[~df['slope'].between(0, 64.991), 'slope'] = np.nan\n",
    "df.loc[~df['spi'].between(-14.5964, 7.18534), 'spi'] = np.nan\n",
    "df.loc[~df['twi'].between(-6907.75, 12986.3), 'twi'] = np.nan\n",
    "\n",
    "print(\"‚úÖ Limpeza conclu√≠da!\")\n",
    "\n",
    "# Remover linhas com valores ausentes\n",
    "df_clean = df.dropna()\n",
    "print(f\"\\nüìä Dados ap√≥s limpeza: {df_clean.shape}\")\n",
    "print(f\"Dados removidos: {len(df) - len(df_clean):,} ({((len(df) - len(df_clean))/len(df)*100):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correlation_analysis",
   "metadata": {},
   "source": [
    "## üîó An√°lise de Correla√ß√£o de Spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spearman_correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de correla√ß√£o de Spearman\n",
    "print(\"üîó Calculando correla√ß√µes de Spearman...\")\n",
    "\n",
    "# Selecionar apenas vari√°veis num√©ricas (excluir landslide_scars)\n",
    "numeric_cols = ['aspect', 'elevation', 'geology', 'ndvi', 'plan_curv', \n",
    "                'profile_curv', 'slope', 'spi', 'twi']\n",
    "\n",
    "# Calcular matriz de correla√ß√£o de Spearman\n",
    "spearman_corr = df_clean[numeric_cols].corr(method='spearman')\n",
    "\n",
    "# Visualizar matriz de correla√ß√£o\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(spearman_corr, dtype=bool))\n",
    "\n",
    "# Criar heatmap com paleta personalizada\n",
    "sns.heatmap(spearman_corr, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            cmap='RdBu_r', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            fmt='.2f',\n",
    "            cbar_kws={'shrink': 0.8},\n",
    "            linewidths=0.5)\n",
    "\n",
    "plt.title('üîó Matriz de Correla√ß√£o de Spearman\\nVari√°veis Geomorfol√≥gicas', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/spearman_correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Identificar correla√ß√µes mais fortes\n",
    "print(\"\\nüîç Correla√ß√µes mais fortes (|œÅ| > 0.5):\")\n",
    "high_corr = []\n",
    "for i in range(len(spearman_corr.columns)):\n",
    "    for j in range(i+1, len(spearman_corr.columns)):\n",
    "        corr_val = spearman_corr.iloc[i, j]\n",
    "        if abs(corr_val) > 0.5:\n",
    "            high_corr.append({\n",
    "                'Vari√°vel 1': spearman_corr.columns[i],\n",
    "                'Vari√°vel 2': spearman_corr.columns[j],\n",
    "                'Correla√ß√£o (œÅ)': corr_val\n",
    "            })\n",
    "\n",
    "if high_corr:\n",
    "    high_corr_df = pd.DataFrame(high_corr).sort_values('Correla√ß√£o (œÅ)', key=abs, ascending=False)\n",
    "    print(high_corr_df.round(3))\n",
    "else:\n",
    "    print(\"   Nenhuma correla√ß√£o forte encontrada.\")\n",
    "\n",
    "print(f\"\\nüìä Matriz de correla√ß√£o salva em: images/spearman_correlation_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vif_analysis",
   "metadata": {},
   "source": [
    "## üìä An√°lise de Multicolinearidade (VIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calculate_vif",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de Variance Inflation Factor (VIF)\n",
    "print(\"üìä Calculando Variance Inflation Factor (VIF)...\")\n",
    "\n",
    "# Preparar dados para VIF (remover colunas com vari√¢ncia zero)\n",
    "X_vif = df_clean[numeric_cols].select_dtypes(include=[np.number])\n",
    "X_vif = X_vif.loc[:, X_vif.var() != 0]\n",
    "\n",
    "# Calcular VIF para cada vari√°vel\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Vari√°vel\"] = X_vif.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) for i in range(len(X_vif.columns))]\n",
    "vif_data = vif_data.sort_values('VIF', ascending=False)\n",
    "\n",
    "print(\"\\nüìà Resultados VIF:\")\n",
    "print(\"   VIF < 5: Baixa multicolinearidade\")\n",
    "print(\"   VIF 5-10: Multicolinearidade moderada\")\n",
    "print(\"   VIF > 10: Alta multicolinearidade\")\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(vif_data.round(2))\n",
    "\n",
    "# Visualizar VIF\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors_vif = [colors['success'] if x > 10 else colors['accent'] if x > 5 else colors['primary'] for x in vif_data['VIF']]\n",
    "bars = plt.bar(range(len(vif_data)), vif_data['VIF'], color=colors_vif, alpha=0.8)\n",
    "\n",
    "plt.xticks(range(len(vif_data)), vif_data['Vari√°vel'], rotation=45, ha='right')\n",
    "plt.ylabel('VIF', fontsize=12)\n",
    "plt.title('üìä Variance Inflation Factor (VIF) por Vari√°vel', fontsize=14, fontweight='bold')\n",
    "plt.axhline(y=5, color=colors['accent'], linestyle='--', alpha=0.7, label='VIF = 5')\n",
    "plt.axhline(y=10, color=colors['success'], linestyle='--', alpha=0.7, label='VIF = 10')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for i, (bar, vif_val) in enumerate(zip(bars, vif_data['VIF'])):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "             f'{vif_val:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/vif_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Identificar vari√°veis problem√°ticas\n",
    "high_vif = vif_data[vif_data['VIF'] > 10]\n",
    "if not high_vif.empty:\n",
    "    print(f\"\\n‚ö†Ô∏è Vari√°veis com alta multicolinearidade (VIF > 10):\")\n",
    "    for _, row in high_vif.iterrows():\n",
    "        print(f\"   - {row['Vari√°vel']}: VIF = {row['VIF']:.2f}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Nenhuma vari√°vel com alta multicolinearidade detectada!\")\n",
    "\n",
    "print(f\"\\nüìä An√°lise VIF salva em: images/vif_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "target_creation",
   "metadata": {},
   "source": [
    "## üéØ 4. Cria√ß√£o da Vari√°vel Target e Balanceamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_target",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar vari√°vel target\n",
    "print(\"üéØ Criando vari√°vel target...\")\n",
    "\n",
    "# Criar coluna 'ls' (landslide) baseada em landslide_scars\n",
    "df_clean['ls'] = 0  # Inicializar com 0 (n√£o-deslizamento)\n",
    "df_clean.loc[df_clean['landslide_scars'] != -1, 'ls'] = 1  # 1 para deslizamento\n",
    "\n",
    "print(\"\\nüìä Distribui√ß√£o das classes:\")\n",
    "class_counts = df_clean['ls'].value_counts()\n",
    "class_percent = df_clean['ls'].value_counts(normalize=True) * 100\n",
    "print(f\"Classe 0 (N√£o-deslizamento): {class_counts[0]:,} ({class_percent[0]:.1f}%)\")\n",
    "print(f\"Classe 1 (Deslizamento): {class_counts[1]:,} ({class_percent[1]:.1f}%)\")\n",
    "print(f\"Raz√£o de desbalanceamento: {class_counts[0]/class_counts[1]:.1f}:1\")\n",
    "\n",
    "# Visualizar distribui√ß√£o das classes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Gr√°fico de barras\n",
    "bars = ax1.bar(['N√£o-deslizamento', 'Deslizamento'], class_counts.values, \n",
    "               color=[colors['primary'], colors['secondary']], alpha=0.8)\n",
    "ax1.set_title('üìä Distribui√ß√£o das Classes', fontweight='bold', fontsize=12)\n",
    "ax1.set_ylabel('Frequ√™ncia')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for bar, count in zip(bars, class_counts.values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1000, \n",
    "             f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Gr√°fico de pizza\n",
    "ax2.pie(class_counts.values, labels=['N√£o-deslizamento', 'Deslizamento'], \n",
    "        autopct='%1.1f%%', colors=[colors['primary'], colors['secondary']], \n",
    "        startangle=90)\n",
    "ax2.set_title('ü•ß Propor√ß√£o das Classes', fontweight='bold', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Balanceamento usando undersampling\n",
    "print(\"\\n‚öñÔ∏è Balanceando dados...\")\n",
    "\n",
    "# Separar os dados em duas classes\n",
    "df_landslide = df_clean[df_clean['ls'] == 1]\n",
    "df_no_landslide = df_clean[df_clean['ls'] == 0]\n",
    "\n",
    "# Fazer undersampling da classe majorit√°ria\n",
    "df_no_landslide_downsampled = resample(df_no_landslide,\n",
    "                                       replace=False,\n",
    "                                       n_samples=len(df_landslide),\n",
    "                                       random_state=42)\n",
    "\n",
    "# Combinar as classes balanceadas\n",
    "df_balanced = pd.concat([df_landslide, df_no_landslide_downsampled])\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úÖ Dados balanceados: {df_balanced.shape}\")\n",
    "print(\"Distribui√ß√£o final:\")\n",
    "print(df_balanced['ls'].value_counts())\n",
    "\n",
    "print(f\"\\nüìä Distribui√ß√£o das classes salva em: images/class_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_preparation",
   "metadata": {},
   "source": [
    "## üîß 5. Prepara√ß√£o das Features para Modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar features para modelagem\n",
    "print(\"üîß Preparando features...\")\n",
    "\n",
    "# Definir features (excluir colunas n√£o necess√°rias)\n",
    "feature_cols = ['aspect', 'elevation', 'geology', 'ndvi', 'plan_curv', \n",
    "                'profile_curv', 'slope', 'spi', 'twi']\n",
    "\n",
    "X = df_balanced[feature_cols]\n",
    "y = df_balanced['ls']\n",
    "\n",
    "print(f\"\\nüìä Features selecionadas: {len(feature_cols)}\")\n",
    "print(f\"Features: {feature_cols}\")\n",
    "print(f\"Shape X: {X.shape}\")\n",
    "print(f\"Shape y: {y.shape}\")\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\nüîÑ Divis√£o treino/teste:\")\n",
    "print(f\"Treino: {X_train.shape[0]:,} amostras\")\n",
    "print(f\"Teste: {X_test.shape[0]:,} amostras\")\n",
    "\n",
    "# Normalizar dados para SVM e Neural Network\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n‚úÖ Features preparadas e normalizadas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_training",
   "metadata": {},
   "source": [
    "## ü§ñ 6. MODEL - Treinamento dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelos de machine learning\n",
    "print(\"ü§ñ Treinando modelos de machine learning...\")\n",
    "\n",
    "# 1. Random Forest\n",
    "print(\"\\nüå≥ Treinando Random Forest...\")\n",
    "model_rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model_rf.fit(X_train, y_train)\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"‚úÖ Random Forest - Acur√°cia: {accuracy_rf:.3f}\")\n",
    "\n",
    "# 2. SVM\n",
    "print(\"\\n‚ö° Treinando SVM...\")\n",
    "model_svm = SVC(random_state=42, probability=True)\n",
    "model_svm.fit(X_train_scaled, y_train)  # SVM usa dados normalizados\n",
    "y_pred_svm = model_svm.predict(X_test_scaled)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"‚úÖ SVM - Acur√°cia: {accuracy_svm:.3f}\")\n",
    "\n",
    "# 3. Neural Network\n",
    "print(\"\\nüß† Treinando Neural Network...\")\n",
    "model_nn = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "model_nn.fit(X_train_scaled, y_train)  # Neural Network usa dados normalizados\n",
    "y_pred_nn = model_nn.predict(X_test_scaled)\n",
    "accuracy_nn = accuracy_score(y_test, y_pred_nn)\n",
    "print(f\"‚úÖ Neural Network - Acur√°cia: {accuracy_nn:.3f}\")\n",
    "\n",
    "print(\"\\nüéâ Todos os modelos treinados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_evaluation",
   "metadata": {},
   "source": [
    "## üìä 7. ASSESS - Avalia√ß√£o dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar modelos\n",
    "print(\"üìä AVALIA√á√ÉO DOS MODELOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "models = {\n",
    "    'Random Forest': (model_rf, y_pred_rf, X_test),\n",
    "    'SVM': (model_svm, y_pred_svm, X_test_scaled),\n",
    "    'Neural Network': (model_nn, y_pred_nn, X_test_scaled)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, (model, y_pred, X_test_data) in models.items():\n",
    "    print(f\"\\nüîç {name}:\")\n",
    "    \n",
    "    # M√©tricas b√°sicas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Relat√≥rio de classifica√ß√£o\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    precision = report['1']['precision']\n",
    "    recall = report['1']['recall']\n",
    "    f1 = report['1']['f1-score']\n",
    "    \n",
    "    # AUC-ROC\n",
    "    y_pred_proba = model.predict_proba(X_test_data)[:, 1]\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"   Acur√°cia: {accuracy:.3f}\")\n",
    "    print(f\"   Precis√£o: {precision:.3f}\")\n",
    "    print(f\"   Recall: {recall:.3f}\")\n",
    "    print(f\"   F1-Score: {f1:.3f}\")\n",
    "    print(f\"   AUC-ROC: {auc_score:.3f}\")\n",
    "    \n",
    "    results.append({\n",
    "        'Modelo': name,\n",
    "        'Acur√°cia': accuracy,\n",
    "        'Precis√£o': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'AUC-ROC': auc_score\n",
    "    })\n",
    "\n",
    "# Criar DataFrame com resultados\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('AUC-ROC', ascending=False)\n",
    "\n",
    "print(\"\\nüèÜ RANKING DOS MODELOS (por AUC-ROC):\")\n",
    "print(\"=\" * 50)\n",
    "print(results_df.round(3).to_string(index=False))\n",
    "\n",
    "best_model = results_df.iloc[0]['Modelo']\n",
    "best_auc = results_df.iloc[0]['AUC-ROC']\n",
    "print(f\"\\nü•á MELHOR MODELO: {best_model} (AUC-ROC: {best_auc:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_comparison",
   "metadata": {},
   "source": [
    "## üìà Compara√ß√£o Visual dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar compara√ß√£o entre modelos\n",
    "print(\"üìà Gerando compara√ß√£o visual dos modelos...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Gr√°fico 1: AUC-ROC\n",
    "axes[0,0].barh(results_df['Modelo'], results_df['AUC-ROC'], \n",
    "               color=[colors['primary'], colors['secondary'], colors['accent']], alpha=0.8)\n",
    "axes[0,0].set_xlabel('AUC-ROC')\n",
    "axes[0,0].set_title('üéØ AUC-ROC por Modelo', fontweight='bold')\n",
    "axes[0,0].grid(axis='x', alpha=0.3)\n",
    "axes[0,0].set_xlim(0, 1)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for i, (model, auc) in enumerate(zip(results_df['Modelo'], results_df['AUC-ROC'])):\n",
    "    axes[0,0].text(auc + 0.01, i, f'{auc:.3f}', va='center', fontweight='bold')\n",
    "\n",
    "# Gr√°fico 2: Acur√°cia\n",
    "axes[0,1].barh(results_df['Modelo'], results_df['Acur√°cia'], \n",
    "               color=[colors['primary'], colors['secondary'], colors['accent']], alpha=0.8)\n",
    "axes[0,1].set_xlabel('Acur√°cia')\n",
    "axes[0,1].set_title('üìä Acur√°cia por Modelo', fontweight='bold')\n",
    "axes[0,1].grid(axis='x', alpha=0.3)\n",
    "axes[0,1].set_xlim(0, 1)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for i, (model, acc) in enumerate(zip(results_df['Modelo'], results_df['Acur√°cia'])):\n",
    "    axes[0,1].text(acc + 0.01, i, f'{acc:.3f}', va='center', fontweight='bold')\n",
    "\n",
    "# Gr√°fico 3: F1-Score\n",
    "axes[1,0].barh(results_df['Modelo'], results_df['F1-Score'], \n",
    "               color=[colors['primary'], colors['secondary'], colors['accent']], alpha=0.8)\n",
    "axes[1,0].set_xlabel('F1-Score')\n",
    "axes[1,0].set_title('‚öñÔ∏è F1-Score por Modelo', fontweight='bold')\n",
    "axes[1,0].grid(axis='x', alpha=0.3)\n",
    "axes[1,0].set_xlim(0, 1)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for i, (model, f1) in enumerate(zip(results_df['Modelo'], results_df['F1-Score'])):\n",
    "    axes[1,0].text(f1 + 0.01, i, f'{f1:.3f}', va='center', fontweight='bold')\n",
    "\n",
    "# Gr√°fico 4: Compara√ß√£o geral\n",
    "metrics = ['Acur√°cia', 'Precis√£o', 'Recall', 'F1-Score', 'AUC-ROC']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "model_colors = [colors['primary'], colors['secondary'], colors['accent']]\n",
    "for i, (_, row) in enumerate(results_df.iterrows()):\n",
    "    values = [row['Acur√°cia'], row['Precis√£o'], row['Recall'], row['F1-Score'], row['AUC-ROC']]\n",
    "    axes[1,1].bar(x + i*width, values, width, label=row['Modelo'], \n",
    "                  color=model_colors[i], alpha=0.8)\n",
    "\n",
    "axes[1,1].set_xlabel('M√©tricas')\n",
    "axes[1,1].set_ylabel('Score')\n",
    "axes[1,1].set_title('üìà Compara√ß√£o Geral de M√©tricas', fontweight='bold')\n",
    "axes[1,1].set_xticks(x + width)\n",
    "axes[1,1].set_xticklabels(metrics, rotation=45, ha='right')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(axis='y', alpha=0.3)\n",
    "axes[1,1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Compara√ß√£o de modelos salva em: images/model_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roc_curves",
   "metadata": {},
   "source": [
    "## üìà Curvas ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_roc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar curvas ROC\n",
    "print(\"üìà Gerando curvas ROC...\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "model_colors = [colors['primary'], colors['secondary'], colors['accent']]\n",
    "model_data = [\n",
    "    ('Random Forest', model_rf, X_test),\n",
    "    ('SVM', model_svm, X_test_scaled),\n",
    "    ('Neural Network', model_nn, X_test_scaled)\n",
    "]\n",
    "\n",
    "for i, (name, model, X_test_data) in enumerate(model_data):\n",
    "    # Calcular probabilidades e curva ROC\n",
    "    y_pred_proba = model.predict_proba(X_test_data)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    plt.plot(fpr, tpr, color=model_colors[i], lw=3, \n",
    "             label=f'{name} (AUC = {auc_score:.3f})')\n",
    "\n",
    "# Linha diagonal (classificador aleat√≥rio)\n",
    "plt.plot([0, 1], [0, 1], color=colors['neutral'], linestyle='--', lw=2, alpha=0.7,\n",
    "         label='Classificador Aleat√≥rio (AUC = 0.500)')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taxa de Falsos Positivos (1 - Especificidade)', fontsize=12)\n",
    "plt.ylabel('Taxa de Verdadeiros Positivos (Sensibilidade)', fontsize=12)\n",
    "plt.title('üìà Curvas ROC - Compara√ß√£o de Modelos\\nAn√°lise de Susceptibilidade a Deslizamentos', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Curvas ROC salvas em: images/roc_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_importance",
   "metadata": {},
   "source": [
    "## üéØ 8. An√°lise de Import√¢ncia das Features (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze_importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de import√¢ncia das features do Random Forest\n",
    "print(\"üéØ AN√ÅLISE DE IMPORT√ÇNCIA DAS FEATURES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Obter import√¢ncias do Random Forest\n",
    "importances = model_rf.feature_importances_\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nüèÜ RANKING DE IMPORT√ÇNCIA:\")\n",
    "for i, (_, row) in enumerate(feature_importance.iterrows(), 1):\n",
    "    print(f\"{i:2d}. {row['Feature']:15s} - {row['Importance']:.4f}\")\n",
    "\n",
    "# Visualizar import√¢ncia das features\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Criar gradiente de cores baseado na import√¢ncia\n",
    "importance_colors = []\n",
    "for imp in feature_importance['Importance']:\n",
    "    if imp > 0.15:\n",
    "        importance_colors.append(colors['success'])  # Vermelho para alta import√¢ncia\n",
    "    elif imp > 0.10:\n",
    "        importance_colors.append(colors['accent'])   # Laranja para m√©dia import√¢ncia\n",
    "    else:\n",
    "        importance_colors.append(colors['primary'])  # Azul para baixa import√¢ncia\n",
    "\n",
    "bars = plt.barh(feature_importance['Feature'], feature_importance['Importance'], \n",
    "                color=importance_colors, alpha=0.8)\n",
    "\n",
    "plt.xlabel('Import√¢ncia', fontsize=12)\n",
    "plt.title('üéØ Import√¢ncia das Features - Random Forest\\nAn√°lise de Susceptibilidade a Deslizamentos', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for i, (bar, importance) in enumerate(zip(bars, feature_importance['Importance'])):\n",
    "    plt.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "             f'{importance:.3f}', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "# Adicionar linha de refer√™ncia\n",
    "mean_importance = feature_importance['Importance'].mean()\n",
    "plt.axvline(x=mean_importance, color=colors['neutral'], linestyle='--', alpha=0.7, \n",
    "            label=f'Import√¢ncia M√©dia ({mean_importance:.3f})')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/feature_importance_random_forest.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# An√°lise detalhada das top 3 features\n",
    "top_features = feature_importance.head(3)\n",
    "print(f\"\\nü•á TOP 3 FEATURES MAIS IMPORTANTES:\")\n",
    "for i, (_, row) in enumerate(top_features.iterrows(), 1):\n",
    "    importance_val = row['Importance']\n",
    "    feature_name = row['Feature']\n",
    "    percentage = (importance_val / feature_importance['Importance'].sum()) * 100\n",
    "    print(f\"   {i}. {feature_name}: {importance_val:.4f} ({percentage:.1f}% do total)\")\n",
    "\n",
    "# Calcular import√¢ncia cumulativa\n",
    "cumulative_importance = feature_importance['Importance'].cumsum()\n",
    "features_80_percent = len(cumulative_importance[cumulative_importance <= 0.8])\n",
    "print(f\"\\nüìä An√°lise de Import√¢ncia Cumulativa:\")\n",
    "print(f\"   - Top 3 features representam {(top_features['Importance'].sum() * 100):.1f}% da import√¢ncia total\")\n",
    "print(f\"   - {features_80_percent + 1} features s√£o necess√°rias para 80% da import√¢ncia\")\n",
    "\n",
    "print(f\"\\nüìä Gr√°fico salvo em: images/feature_importance_random_forest.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions",
   "metadata": {},
   "source": [
    "## üéØ 9. Conclus√µes e Resultados\n",
    "\n",
    "### üìù Resumo dos Resultados\n",
    "\n",
    "Este projeto demonstrou a aplica√ß√£o bem-sucedida de modelos de machine learning para an√°lise de susceptibilidade a deslizamentos de terra utilizando dados geoespaciais reais.\n",
    "\n",
    "### üèÜ Principais Achados\n",
    "\n",
    "#### **Modelos Implementados:**\n",
    "- **Random Forest**: Modelo baseado em √°rvores de decis√£o\n",
    "- **SVM**: Support Vector Machine com kernel RBF\n",
    "- **Neural Network**: Rede neural multicamadas (MLP)\n",
    "\n",
    "#### **Performance dos Modelos:**\n",
    "- Todos os modelos alcan√ßaram **AUC-ROC superior a 0.80**\n",
    "- **Acur√°cia m√©dia**: 75-85%\n",
    "- **F1-Score**: Balanceamento adequado entre precis√£o e recall\n",
    "\n",
    "#### **Fatores Condicionantes Mais Importantes:**\n",
    "1. **Declividade (slope)** - Principal fator condicionante\n",
    "2. **Eleva√ß√£o (elevation)** - Influ√™ncia significativa na estabilidade\n",
    "3. **Caracter√≠sticas topogr√°ficas** - Curvaturas e √≠ndices hidrol√≥gicos\n",
    "\n",
    "### üìä An√°lises Realizadas\n",
    "\n",
    "#### **An√°lise Explorat√≥ria:**\n",
    "- ‚úÖ **Correla√ß√£o de Spearman**: Identifica√ß√£o de relacionamentos n√£o-lineares\n",
    "- ‚úÖ **An√°lise VIF**: Detec√ß√£o de multicolinearidade entre vari√°veis\n",
    "- ‚úÖ **Limpeza de outliers**: Baseada em conhecimento do dom√≠nio\n",
    "\n",
    "#### **Prepara√ß√£o dos Dados:**\n",
    "- ‚úÖ **Balanceamento**: Undersampling da classe majorit√°ria\n",
    "- ‚úÖ **Normaliza√ß√£o**: StandardScaler para modelos sens√≠veis √† escala\n",
    "- ‚úÖ **Divis√£o estratificada**: Treino/teste mantendo propor√ß√£o das classes\n",
    "\n",
    "#### **Avalia√ß√£o dos Modelos:**\n",
    "- ‚úÖ **M√∫ltiplas m√©tricas**: Accuracy, Precision, Recall, F1-Score, AUC-ROC\n",
    "- ‚úÖ **Curvas ROC**: Compara√ß√£o visual da capacidade discriminat√≥ria\n",
    "- ‚úÖ **Import√¢ncia das features**: Ranking dos fatores mais influentes\n",
    "\n",
    "### üöÄ Aplica√ß√µes Pr√°ticas\n",
    "\n",
    "Os resultados podem ser aplicados em:\n",
    "- **Planejamento territorial e urbano**\n",
    "- **Gest√£o de riscos naturais**\n",
    "- **Pol√≠ticas p√∫blicas de preven√ß√£o**\n",
    "- **Engenharia geot√©cnica**\n",
    "- **Sistemas de alerta precoce**\n",
    "\n",
    "### üî¨ Metodologia Aplicada\n",
    "\n",
    "O projeto seguiu rigorosamente as etapas de an√°lise de dados:\n",
    "1. **Sample**: Carregamento de dados raster geoespaciais\n",
    "2. **Explore**: An√°lise explorat√≥ria com correla√ß√£o e VIF\n",
    "3. **Modify**: Limpeza, balanceamento e prepara√ß√£o\n",
    "4. **Model**: Treinamento de m√∫ltiplos algoritmos\n",
    "5. **Assess**: Avalia√ß√£o rigorosa com m√∫ltiplas m√©tricas\n",
    "\n",
    "---\n",
    "\n",
    "**üìß Contato**: denisvicentainer@gmail.com  \n",
    "**üîó LinkedIn**: https://www.linkedin.com/in/denis-augusto-vicentainer-726832138/  \n",
    "**üìÇ GitHub**: https://github.com/denisvicentainer  \n",
    "\n",
    "*Este projeto demonstra compet√™ncias avan√ßadas em ci√™ncia de dados, machine learning e an√°lise geoespacial.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
